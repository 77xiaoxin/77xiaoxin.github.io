---
layout: post
category: hadoop云计算
title: CDH5 NameNode HA实践
tags: [hadoop]
---
{% include JB/setup %}

###准备

虚拟设备4台，安装CentOS6.5操作系统，每台设备内存1G，分别命名为：bigdata1, bigdata2, bigdata3, bigdata4.

下载<a target="_blank" href="http://archive.cloudera.com/cdh5/one-click-install/redhat/6/x86_64/cloudera-cdh-5-0.x86_64.rpm">cloudera-cdh-5-0.x86_64.rpm</a>.

###规划

zookeeper: bigdata1, bigdata2, bigdata3.

NameNode: bigdata1, bigdata2.

DataNode: all.

ResourceManager: bigdata3.

JournalNode： bigdata1, bigdata2, bigdata3.

###安装

在四台虚拟机上安装下载好的cloudera-cdh-5.0.x86_64.rpm：

	yum --nogpgcheck localinstall cloudera-cdh-5-0.x86_64.rpm

导入仓库证书（可选）：

	rpm --import http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera

安装zookeeper是配置NameNode HA所必须的，在三台设备上分别运行如下命令：

	yum install zookeeper zookeeper-server

修改zookeeper配置文件`/etc/zookeeper/conf/zoo.cfg`：

	tickTime=2000
	dataDir=/var/lib/zookeeper/
	clientPort=2181
	initLimit=5
	syncLimit=2
	server.1=bigdata1:2888:3888
	server.2=bigdata2:2888:3888
	server.3=bigdata3:2888:3888

在`/var/lib/zookeeper/`目录下创建myid文件，注意权限配置为zookeeper:zookeeper，myid文件中存储的是一个1-255之间的整数（可选）。

在3台设备分别执行：

	/etc/init.d/zookeeper-server init --myid=1
	/etc/init.d/zookeeper-server init --myid=2
	/etc/init.d/zookeeper-server init --myid=3

指定myid唯一。

启动zookeeper：

	/etc/init.d/zookeeper-server start

查看zookeeper状态：

	zookeeper-server status

在NameNode节点运行：

	yum install hadoop-hdfs-namenode
	yum install hadoop-hdfs-zkfc

在ResourceManager节点运行：

	yum install hadoop-yarn-resourcemanager

在DataNode节点运行：

	yum install hadoop-yarn-nodemanager hadoop-hdfs-datanode hadoop-mapreduce

在其中1台设备上运行：

	yum install hadoop-mapreduce-historyserver hadoop-yarn-proxyserver

在所有设备上运行：

	yum install hadoop-client

在JournalNode节点运行：

	yum install hadoop-hdfs-journalnode

配置`core-site.xml`：

	<configuration>
	  <property>
	    <name>fs.defaultFS</name>
	    <value>hdfs://sep10</value>
	  </property>

	  <property>
	    <name>hadoop.proxyuser.httpfs.hosts</name>
	    <value>*</value>
	  </property>

	  <property>
	    <name>hadoop.proxyuser.httpfs.groups</name>
	    <value>*</value>
	  </property>
	</configuration>

配置`hdfs-site.xml`：

	<configuration>
	  <property>
	    <name>dfs.nameservices</name>
	    <value>sep10</value>
	  </property>

	  <property>
	    <name>dfs.ha.namenodes.sep10</name>
	    <value>nn1,nn2</value>
	  </property>

	  <property>
	    <name>dfs.namenode.rpc-address.sep10.nn1</name>
	    <value>bigdata1:8020</value>
	  </property>

	  <property>
	    <name>dfs.namenode.rpc-address.sep10.nn2</name>
	    <value>bigdata2:8020</value>
	  </property>

	  <property>
	    <name>dfs.namenode.http-address.sep10.nn1</name>
	    <value>bigdata1:50070</value>
	  </property>

	  <property>
	    <name>dfs.namenode.http-address.sep10.nn2</name>
	    <value>bigdata2:50070</value>
	  </property>

	  <property>
	    <name>dfs.namenode.shared.edits.dir</name>
	    <value>qjournal://bigdata1:8485;bigdata2:8485;bigdata3:8485/sep10</value>
	  </property>

	  <property>
	    <name>dfs.journalnode.edits.dir</name>
	    <value>/hadoop/hdfs/jn</value>
	  </property>

	  <property>
	    <name>dfs.client.failover.proxy.provider.sep10</name>
	    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	  </property>

	  <property>
	    <name>dfs.ha.fencing.methods</name>
	    <value>sshfence</value>
	  </property>
	
	  <property>
	    <name>dfs.ha.fencing.ssh.private-key-files</name>
	    <value>/root/.ssh/id_rsa</value>
	  </property>

	  <property>
	    <name>dfs.ha.automatic-failover.enabled</name>
	    <value>true</value>
	  </property>

	  <property>
	    <name>ha.zookeeper.quorum</name>
	    <value>bigdata1:2181,bigdata2:2181,bigdata3:2181</value>
	  </property>

	  <property>
	     <name>dfs.namenode.name.dir</name>
	     <value>/hadoop/hdfs/namenode</value>
	  </property>
	
	  <property>
	    <name>dfs.datanode.data.dir</name>
	    <value>/hadoop/hdfs/datanode</value>
	  </property>
	
	  <property>
	    <name>dfs.webhdfs.enabled</name>
	    <value>true</value>
	  </property>
	  
	  <property>
	    <name>dfs.permissions.superusergroup</name>
	    <value>hadoop</value>
	  <property>
	</configuration>

配置`mapred-site.xml`：

	<configuration>
	  <property>
	    <name>mapreduce.framework.name</name>
	    <value>yarn</value>
	  </property>

	  <property>
	    <name>mapreduce.jobhistory.address</name>
	    <value>bigdata4:10020</value>
	  </property>

	  <property>
	    <name>mapreduce.jobhistory.webapp.address</name>
	    <value>bigdata4:19888</value>
	  </property> 
	</configuration>

配置`yarn-site.xml`：

	<configuration>
	  <property>
	    <name>yarn.nodemanager.aux-services</name>
	    <value>mapreduce_shuffle</value>
	  </property>

	  <property>
	    <name>yarn.resourcemanager.hostname</name>
	    <value>bigdata3</value>
	  </property>

	  <property>
	    <name>yarn.application.classpath</name>
	    <value>
	        $HADOOP_CONF_DIR,
	        $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
	        $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
	        $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
	        $YARN_HOME/*,$YARN_HOME/lib/*
	    </value>
	  </property>

	  <property>
	    <name>yarn.log.aggregation.enable</name>
	    <value>true</value> 
	  </property>

	  <property>
	    <name>yarn.nodemanager.local-dirs</name>
	    <value>/hadoop/yarn/local</value>
	  </property>

	  <property>
	    <name>yarn.nodemanager.log-dirs</name>
	    <value>/hadoop/yarn/logs</value>
	  </property>

	  <property>
	    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
	    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	  </property>

	  <property>
	    <name>yarn.nodemanager.remote-app-log-dir</name>
	    <value>hdfs://var/log/hadoop-yarn/apps</value>
	  </property>
	</configuration>

创建相关文件夹：

NameNode:

	mkdir -p /hadoop/hdfs/namenode
	chown -R hdfs:hdfs /hadoop/hdfs/namenode
	chmod 700 /hadoop/hdfs/namenode

DataNode:

	mkdir -p /hadoop/hdfs/datanode
	chown -R hdfs:hdfs /hadoop/hdfs/datanode
	chmod 700 /hadoop/hdfs/datanode

JournalNode:

	mkdir -p /hadoop/hdfs/jn
	chown -R hdfs:hdfs /hadoop/hdfs/jn

yarn：

	mkdir -p /hadoop/yarn/local
	chown -R yarn:yarn /hadoop/yarn/local

	mkdir -p /hadoop/yarn/logs
	chown -R yarn:yarn /hadoop/yarn/logs

在zookeeper中初始化HA状态：

	hdfs zkfc -formatZK

启动journalnode：

	/etc/init.d/hadoop/hadoop-hdfs-journalnode start

切换到hdfs用户下对NameNode格式化：

	hadoop namenode -format

初始化Shared Edits directory(bigdata1)：

	hdfs namenode -initializeSharedEdits

启动NameNode：

bigdata1:

	/etc/init.d/hadoop-hdfs-namenode start

bigdata2:

	sudo -u hdfs hdfs namenode -bootstrapStandby
	/etc/init.d/hadoop-hdfs-namenode start

启动DataNode：

	/etc/init.d/hadoop-hdfs-datenode start

启动ZKFC：

	/etc/init.d/hadoop-hdfs-zkfc start


切换到hdfs用户，创建相关目录：

	hadoop fs -mkdir /tmp
	hadoop fs -chmod -R 1777 /tmp

	hadoop fs -mkdir -p /user/history
	hadoop fs -chmod -R 1777 /user/history
	hadoop fs -chown yarn /user/history

	hadoop fs -mkdir -p /var/log/hadoop-yarn
	hadoop fs -chown yarn:mapred /var/log/hadoop-yarn

启动ResourceManager：

	/etc/init.d/hadoop-yarn-resourcemanager start

启动NodeManager：

	/etc/init.d/hadoop-yarn-nodemanager start

启动HistoryServer：

	/etc/init.d/hadoop-mapreduce-historyserver start

为每个MapReduce用户创建home目录

	hadoop fs -mkdir -p /user/$USER
	hadoop fs -chown $USER /user/$USER

在浏览器中访问：http://bigdata1:50070和http://bigdata2:50070，如果显示一个状态为active，另一个状态为standby，则搭建成功。

###测试

在当前状态为active的NameNode上执行：

	/etc/init.d/hadoop-hdfs-namenode stop

通过页面查看另一个NameNode是否状态切换为active。


